---
title: "Machine Learning: Regularized Regressions and Combining Predictors"
author: "J Faleiro"
date: "April 28, 2015"
output: 
    html_document:
        keep_md: true
        toc: true
        theme: united
---

# Required libraries

```{r, warning=FALSE, message=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ElemStatLearn, ISLR, ggplot2, caret, quantmod, dplyr, forecast)
```

# Regulatized Regression

## Example: Prostate Cancer

```{r}
library(ElemStatLearn)
data(prostate)
str(prostate)
```

```{r}
covnames <- names(prostate[-(9:10)])
y <- prostate$lpsa
x <- prostate[,covnames]

form <- as.formula(paste("lpsa~", paste(covnames, collapse="+"), sep=""))
summary(lm(form, data=prostate[prostate$train,]))
```

```{r}
set.seed(1)
train.ind <- sample(nrow(prostate), ceiling(nrow(prostate))/2)
y.test <- prostate$lpsa[-train.ind]
x.test <- x[-train.ind,]

y <- prostate$lpsa[train.ind]
x <- x[train.ind,]

p <- length(covnames)
rss <- list()
for (i in 1:p) {
  cat(i)
  Index <- combn(p,i)

  rss[[i]] <- apply(Index, 2, function(is) {
    form <- as.formula(paste("y~", paste(covnames[is], collapse="+"), sep=""))
    isfit <- lm(form, data=x)
    yhat <- predict(isfit)
    train.rss <- sum((y - yhat)^2)

    yhat <- predict(isfit, newdata=x.test)
    test.rss <- sum((y.test - yhat)^2)
    c(train.rss, test.rss)
  })
}
```

Plotting of residuals by number of predictors:

```{r}
plot(1:p, 1:p, type="n", ylim=range(unlist(rss)), xlim=c(0,p), xlab="number of predictors", ylab="residual sum of squares", main="Prostate cancer data")
for (i in 1:p) {
  points(rep(i-0.15, ncol(rss[[i]])), rss[[i]][1, ], col="blue")
  points(rep(i+0.15, ncol(rss[[i]])), rss[[i]][2, ], col="red")
}
minrss <- sapply(rss, function(x) min(x[1,]))
lines((1:p)-0.15, minrss, col="blue", lwd=1.7)
minrss <- sapply(rss, function(x) min(x[2,]))
lines((1:p)+0.15, minrss, col="red", lwd=1.7)
legend("topright", c("Train", "Test"), col=c("blue", "red"), pch=1)
```

A few important observations:

* The training set error (residuals) always goes down as we increase the number of predictors

* As we increase the number of predictors, residuals on the test set go down and them up - this is an indication of a model overfitting

## High Dimensional Data

Another issue for high dimensional data:

```{r}
small <- prostate[1:5,]
lm(lpsa ~ ., data=small)
```

For small datasets, several of the predictors are irrelevant, marked as `NA`

# Combining Predictors

Also known as ensembling, combinaion of several models.

Example with wage data. Create training, test and validation sets:

```{r}
library(ISLR); data(Wage); library(ggplot2); library(caret)
Wage <- subset(Wage, select=-c(logwage))
inBuild <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
buildData <- Wage[inBuild,]
validation <- Wage[-inBuild,]
inTrain <- createDataPartition(y=buildData$wage, p=0.7, list=FALSE)
training <- Wage[inBuild,]
testing <- Wage[-inBuild,]
```

Build 2 different models, fitting both to the same dataset:

```{r train.combined, cache=TRUE, warning=FALSE, message=FALSE}
mod1 <- train(wage ~ ., method='glm', data=training) # first model GLM
mod2 <- train(wage ~ ., method='rf', data=training, number=3, 
              trControl=trainControl(method='cv')) # second model: random forests
```

Plot 2 predictions:

```{r warning=FALSE, message=FALSE}
pred1 <- predict(mod1, testing)
pred2 <- predict(mod2, testing)
qplot(pred1, pred2, col=wage, data=testing)
```

You can see they do not exactly agree on the prediction, and they also differ from the actual `wage` on the test data set, set in the scale of colours.

We can now fit a model that combines both using `method='gam'`

```{r tranin.combine.both, cache=TRUE, warning=FALSE, message=FALSE}
predDf <- data.frame(pred1, pred2, wage=testing$wage)
combModFit <- train(wage ~ ., method='gam', data=predDf)
```

```{r message=FALSE, warning=FALSE}
combPred <- predict(combModFit, predDf)
```

How well did we do?

```{r}
c(sqrt(sum((pred1 - testing$wage)^2)),
  sqrt(sum((pred2 - testing$wage)^2)),
  sqrt(sum((combPred - testing$wage)^2)))
```

We can see the combined prediction error is slightly lower than the individual predictions.

We can predict on the validation data set as well:

```{r warning=FALSE, message=FALSE}
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
predDfV <- data.frame(pred1=pred1V, pred2=pred2V)
combPredV <- predict(combModFit, predDfV)
```

How well did we do?

```{r}
c(sqrt(sum((pred1V - validation$wage)^2)),
  sqrt(sum((pred2V - validation$wage)^2)),
  sqrt(sum((combPredV - validation$wage)^2)))
```

Again smaller errors on combined predictors. Even simple model blending like this brings benefits.

# Forecasting

```{r}
library(quantmod)
from.dat <- as.Date('01/01/08', format='%m/%d/%y')
to.dat <- as.Date('12/31/15', format='%m/%d/%y')
#getSymbols('GOOG', src='google', from=from.dat, to=to.dat)
getSymbols('GOOG', from=from.dat, to=to.dat)
```

```{r}
head(GOOG)
```

```{r}
tail(GOOG)
```

Summarize monthly and store as time series:

```{r}
mGoog <- to.monthly(GOOG)
head(mGoog)
```

```{r}
googOpen <- Op(mGoog)
ts1 <- ts(googOpen, frequency=12)
plot(ts1, xlab='year+1', ylab='GOOG')
```

Decompose a time series into patterns:

* trends: long term increase/decrease
* seasonal: patterns per week/month/year/etc
* cycles: raises/falls periodically

```{r}
plot(decompose(ts1), xlab='years+1')
```

We can clearly see:

* an upward, and then downward pattern
* a seasonal pattern of ups and downs

When building training and testing sets, we have to maintain the sequence (windows):

```{r}
ts1Train <- window(ts1, start=1, end=5)
ts1Test <- window(ts1, start=5, end=(7-0.1))
ts1Train
```

Exponential smooting: we can smooth the time series with `ets` and forecast future points with `forecast`.

```{r}
library(forecast)
ets1 <- ets(ts1Train, model='MMM')
fcast <- forecast(ets1)
plot(fcast)
lines(ts1Test, col='red')
```

In red, the test set (remember, for timeseries they are taken in contiguous windows), in blue the predicted forecast and the bounds of confidence (shades of grey).

The difference between red and blue is the error, what is quite large:

```{r}
accuracy(fcast, ts1Test)
```

# Unsupervised Prediction

Basic steps:

* Create clusters
* Name clusters (tough)
* Build predictor for clusters

Example: iris dataset, ignoring species labels

```{r}
library(ggplot2)
set.seed(123)
data(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
```

Clustering with k-means:

```{r}
kmeans1 <- kmeans(subset(training, select=-c(Species)), centers=3) # create 3 clusters and ignore species column
training$clusters <- as.factor(kmeans1$cluster)
qplot(Petal.Width, Petal.Length, col=clusters, data=training)
```

How do clusters compare to real labels of species?

```{r}
table(kmeans1$cluster, training$Species)
```

Looking at this classification, we can see that the k-mean created clusters relate to real species in the following way:

* 3 = setosa
* 1 = versicolor
* 2 = virginica

We can now create a predictor:

```{r train.clusters.rpart, cache=TRUE, warning=FALSE, message=FALSE}
modFit <- train(clusters ~ ., data=subset(training, select=-c(Species)), method='rpart')
```

How close did they get?

```{r}
table(predict(modFit, training), training$Species)
```

Pretty close - the misclassifications are on the border area between clusters.

We can apply on the test dataset now and see how it goes:

```{r}
testClusterPred <- predict(modFit, testing)
table(testClusterPred, testing$Species)
```

You can also use `cp_predict` in the `clue` package for similar work.

