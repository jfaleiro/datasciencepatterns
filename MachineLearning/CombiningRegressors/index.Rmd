---
title: "Machine Learning: Regularized Regressions and Combining Predictors"
author: "J Faleiro"
date: "April 28, 2015"
output: 
    html_document:
        keep_md: true
        toc: true
        theme: united
---

# Required libraries

```{r, warning=FALSE, message=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ElemStatLearn, ISLR, ggplot2, caret, quantmod, plyr, dplyr, forecast, 
               elasticnet, lubridate, e1071)
```

# Regularized Regression

## Example: Prostate Cancer

```{r}
library(ElemStatLearn)
data(prostate)
str(prostate)
```

```{r}
covnames <- names(prostate[-(9:10)])
y <- prostate$lpsa
x <- prostate[,covnames]

form <- as.formula(paste("lpsa~", paste(covnames, collapse="+"), sep=""))
summary(lm(form, data=prostate[prostate$train,]))
```

```{r}
set.seed(1)
train.ind <- sample(nrow(prostate), ceiling(nrow(prostate))/2)
y.test <- prostate$lpsa[-train.ind]
x.test <- x[-train.ind,]

y <- prostate$lpsa[train.ind]
x <- x[train.ind,]

p <- length(covnames)
rss <- list()
for (i in 1:p) {
  cat(i)
  Index <- combn(p,i)

  rss[[i]] <- apply(Index, 2, function(is) {
    form <- as.formula(paste("y~", paste(covnames[is], collapse="+"), sep=""))
    isfit <- lm(form, data=x)
    yhat <- predict(isfit)
    train.rss <- sum((y - yhat)^2)

    yhat <- predict(isfit, newdata=x.test)
    test.rss <- sum((y.test - yhat)^2)
    c(train.rss, test.rss)
  })
}
```

Plotting of residuals by number of predictors:

```{r}
plot(1:p, 1:p, type="n", ylim=range(unlist(rss)), xlim=c(0,p), xlab="number of predictors", ylab="residual sum of squares", main="Prostate cancer data")
for (i in 1:p) {
  points(rep(i-0.15, ncol(rss[[i]])), rss[[i]][1, ], col="blue")
  points(rep(i+0.15, ncol(rss[[i]])), rss[[i]][2, ], col="red")
}
minrss <- sapply(rss, function(x) min(x[1,]))
lines((1:p)-0.15, minrss, col="blue", lwd=1.7)
minrss <- sapply(rss, function(x) min(x[2,]))
lines((1:p)+0.15, minrss, col="red", lwd=1.7)
legend("topright", c("Train", "Test"), col=c("blue", "red"), pch=1)
```

A few important observations:

* The training set error (residuals) always goes down as we increase the number of predictors

* As we increase the number of predictors, residuals on the test set go down and them up - this is an indication of a model overfitting

## High Dimensional Data

Another issue for high dimensional data:

```{r}
small <- prostate[1:5,]
lm(lpsa ~ ., data=small)
```

For small datasets, several of the predictors are irrelevant, marked as `NA`

## Example: Concrete LASSO Regression

```{r}
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
```

```{r train.lasso, cache=TRUE, warning=FALSE, message=FALSE}
set.seed(233)
mod <- train(CompressiveStrength ~ ., method='lasso', data=training)
```

Which variable is the last coefficient to be set to zero as the penalty increases? 

```{r}
library(elasticnet)
plot.enet(mod$finalModel, xvar='penalty', use.color=TRUE)
```

The last coefficient to reach zero, the black line, is **Cement**.

## Example: Concrete Strength Prediction & SVM

```{r}
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
```

Set the seed to 325 and fit a support vector machine to predict Compressive Strength using the default settings. Predict on the testing set. 

```{r fit.svm, cache=TRUE}
library(e1071)
set.seed(325)
fit <- svm(CompressiveStrength ~ ., data=training)
```

What is the RMSE? 

```{r}
pred <- predict(fit, newdata=testing)
accuracy(pred, testing$CompressiveStrength)
```

From the table above, **RMSE = 6.715009**

# Combining Predictors

Also known as ensembling, combinaion of several models.

## Example: Wage Prediction

Example with wage data. Create training, test and validation sets:

```{r}
library(ISLR); data(Wage); library(ggplot2); library(caret)
Wage <- subset(Wage, select=-c(logwage))
inBuild <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
buildData <- Wage[inBuild,]
validation <- Wage[-inBuild,]
inTrain <- createDataPartition(y=buildData$wage, p=0.7, list=FALSE)
training <- Wage[inBuild,]
testing <- Wage[-inBuild,]
```

Build 2 different models, fitting both to the same dataset:

```{r train.combined, cache=TRUE, warning=FALSE, message=FALSE}
mod1 <- train(wage ~ ., method='glm', data=training) # first model GLM
mod2 <- train(wage ~ ., method='rf', data=training, number=3, 
              trControl=trainControl(method='cv')) # second model: random forests
```

Plot 2 predictions:

```{r warning=FALSE, message=FALSE}
pred1 <- predict(mod1, testing)
pred2 <- predict(mod2, testing)
qplot(pred1, pred2, col=wage, data=testing)
```

You can see they do not exactly agree on the prediction, and they also differ from the actual `wage` on the test data set, set in the scale of colours.

We can now fit a model that combines both using `method='gam'`

```{r tranin.combine.both, cache=TRUE, warning=FALSE, message=FALSE}
predDf <- data.frame(pred1, pred2, wage=testing$wage)
combModFit <- train(wage ~ ., method='gam', data=predDf)
```

```{r message=FALSE, warning=FALSE}
combPred <- predict(combModFit, predDf)
```

How well did we do?

```{r}
c(sqrt(sum((pred1 - testing$wage)^2)),
  sqrt(sum((pred2 - testing$wage)^2)),
  sqrt(sum((combPred - testing$wage)^2)))
```

We can see the combined prediction error is slightly lower than the individual predictions.

We can predict on the validation data set as well:

```{r warning=FALSE, message=FALSE}
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
predDfV <- data.frame(pred1=pred1V, pred2=pred2V)
combPredV <- predict(combModFit, predDfV)
```

How well did we do?

```{r}
c(sqrt(sum((pred1V - validation$wage)^2)),
  sqrt(sum((pred2V - validation$wage)^2)),
  sqrt(sum((combPredV - validation$wage)^2)))
```

Again smaller errors on combined predictors. Even simple model blending like this brings benefits.

## Example: Vowel Prediction

```{r}
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
```

```{r rf.factor.2, cache=TRUE}
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
```

```{r rf.fit.vowels.2, cache=TRUE, dependson='rf.factor.2', warning=FALSE, message=FALSE}
set.seed(33833) 
mod1 <- train(y ~ ., data=vowel.train, method='rf')
mod2 <- train(y ~ ., data=vowel.train, method='gbm', verbose=FALSE) # gbm() is REALLY verbose...
```

Define predictors

```{r warning=FALSE, message=FALSE}
pred1 <- predict(mod1, vowel.test)
pred2 <- predict(mod2, vowel.test)
```

Plot 2 predictions, just to get a visual sense of how much they agree...

```{r}
qplot(pred1, pred2, col=y, data=vowel.test)
```

What is the accuracy of the random forest model?

```{r}
confusionMatrix(vowel.test$y, pred1)$overall['Accuracy']
```

What is the accuracy of boosted trees?

```{r}
confusionMatrix(vowel.test$y, pred2)$overall['Accuracy']
```

What is the agreement accuracy (i.e. accuracy among the test set samples where the two methods agree)?

```{r}
indexAgreed <- (pred1 == pred2)
confusionMatrix(vowel.test$y[indexAgreed], pred2[indexAgreed])$overall['Accuracy']
```

And the result above should be the same as... (it should be reflexive):

```{r}
indexAgreed <- (pred1 == pred2)
confusionMatrix(vowel.test$y[indexAgreed], pred1[indexAgreed])$overall['Accuracy']
```

## Example: Alzheimer Disease

```{r}
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
```

Define predictors for random forest, boosted trees and linear discriminant analysis:

```{r r.fit.alzheimer, cache=TRUE, message=FALSE, warning=FALSE}
set.seed(62433)
rf.fit <- train(diagnosis ~ ., data=training, method='rf')
gbm.fit <- train(diagnosis ~ ., data=training, method='gbm', verbose=FALSE)
lda.fit <- train(diagnosis ~ ., data=training, method='lda')
```

Predict on testing data set for each model:

```{r}
rf.pred <- predict(rf.fit, testing)
gbm.pred <- predict(gbm.fit, testing)
lda.pred <- predict(lda.fit, testing)
```

Combined data frame for a stacked analysis

```{r}
stacked.df <- data.frame(rf.pred, gbm.pred, lda.pred, diagnosis=testing$diagnosis)
```

Stacked model based on random forests:

```{r stacked.train.alz, cache=TRUE, message=FALSE, warning=FALSE}
stacked.fit <- train(diagnosis ~ ., method='rf', data=stacked.df)
```

Predict testing data set based on the stacked model:

```{r}
stacked.pred <- predict(stacked.fit, testing)
```

What is the accuracy of the random forest model?

```{r}
confusionMatrix(testing$diagnosis, rf.pred)$overall['Accuracy']
```

What is the accuracy of linear discriminant analysis?

```{r}
confusionMatrix(testing$diagnosis, lda.pred)$overall['Accuracy']
```

What is the accuracy of boosted trees?

```{r}
confusionMatrix(testing$diagnosis, gbm.pred)$overall['Accuracy']
```

What is the accuracy of the stacked model?

```{r}
confusionMatrix(testing$diagnosis, stacked.pred)$overall['Accuracy']
```


# Forecasting

## Example: Financial Data

```{r}
library(quantmod)
from.dat <- as.Date('01/01/08', format='%m/%d/%y')
to.dat <- as.Date('12/31/15', format='%m/%d/%y')
#getSymbols('GOOG', src='google', from=from.dat, to=to.dat)
getSymbols('GOOG', from=from.dat, to=to.dat)
```

```{r}
head(GOOG)
```

```{r}
tail(GOOG)
```

Summarize monthly and store as time series:

```{r}
mGoog <- to.monthly(GOOG)
head(mGoog)
```

```{r}
googOpen <- Op(mGoog)
ts1 <- ts(googOpen, frequency=12)
plot(ts1, xlab='year+1', ylab='GOOG')
```

Decompose a time series into patterns:

* trends: long term increase/decrease
* seasonal: patterns per week/month/year/etc
* cycles: raises/falls periodically

```{r}
plot(decompose(ts1), xlab='years+1')
```

We can clearly see:

* an upward, and then downward pattern
* a seasonal pattern of ups and downs

When building training and testing sets, we have to maintain the sequence (windows):

```{r}
ts1Train <- window(ts1, start=1, end=5)
ts1Test <- window(ts1, start=5, end=(7-0.1))
ts1Train
```

## Exponential Smoothing

We can smooth the time series with `ets` and forecast future points with `forecast`.

```{r}
library(forecast)
ets1 <- ets(ts1Train, model='MMM')
fcast <- forecast(ets1)
plot(fcast)
lines(ts1Test, col='red')
```

In red, the test set (remember, for timeseries they are taken in contiguous windows), in blue the predicted forecast and the bounds of confidence (shades of grey).

The difference between red and blue is the error, what is quite large:

```{r}
accuracy(fcast, ts1Test)
```

## Example: Site Visits

```{r download.site.visit, cache=TRUE}
library(lubridate) # For year() function below
temporaryFile <- tempfile()
url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv'
download.file(url, destfile=temporaryFile, method="curl")
dat = read.csv(temporaryFile)
```

```{r}
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
```

Training with a `bats` function: BATS model (Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components)

```{r train.bats, cache=TRUE}
mod <- bats(tstrain)
```

Forecast:

```{r}
fcast95 <- forecast.bats(mod, h=nrow(testing), level=95) # 95% prediction interval
```

For how many of the testing points is the true value within the 95% prediction interval bounds?

```{r}
withinRangeCount <- 0
for (i in 1:nrow(testing)) {
    trueValue <- testing$visitsTumblr[i]
    if (fcast95$lower[i] < trueValue & trueValue < fcast95$upper[i]) {
        withinRangeCount <- withinRangeCount + 1
    }
}
withinRangeCount
```

And what is the percentage in relation to testing sample count?

```{r}
withinRangeCount/nrow(testing)
```

i.e. approx **96.17%**

# Unsupervised Prediction

Basic steps:

* Create clusters
* Name clusters (tough)
* Build predictor for clusters

Example: iris dataset, ignoring species labels

```{r}
library(ggplot2)
set.seed(123)
data(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
```

Clustering with k-means:

```{r}
kmeans1 <- kmeans(subset(training, select=-c(Species)), centers=3) # create 3 clusters and ignore species column
training$clusters <- as.factor(kmeans1$cluster)
qplot(Petal.Width, Petal.Length, col=clusters, data=training)
```

How do clusters compare to real labels of species?

```{r}
table(kmeans1$cluster, training$Species)
```

Looking at this classification, we can see that the k-mean created clusters relate to real species in the following way:

* 3 = setosa
* 1 = versicolor
* 2 = virginica

We can now create a predictor:

```{r train.clusters.rpart, cache=TRUE, warning=FALSE, message=FALSE}
modFit <- train(clusters ~ ., data=subset(training, select=-c(Species)), method='rpart')
```

How close did they get?

```{r}
table(predict(modFit, training), training$Species)
```

Pretty close - the misclassifications are on the border area between clusters.

We can apply on the test dataset now and see how it goes:

```{r}
testClusterPred <- predict(modFit, testing)
table(testClusterPred, testing$Species)
```

You can also use `cp_predict` in the `clue` package for similar work.

